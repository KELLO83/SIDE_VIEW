import os
import cv2
import numpy as np
from ultralytics import YOLO
from tqdm.auto import  tqdm
import os
import logging
import pandas as pd
import functools
import time
import fisheye2plane
import fisheye2plane_GPU
import torch
from natsort import natsorted
from glob import glob
from typing import Union
from collections import Counter
import multiprocessing as mp
import traceback
from sit_recognition import SitRecognition
from typing import Dict, List, Tuple, Optional, Union
from collections import deque
import math
import random
import matplotlib
from dataclasses import dataclass
from typing import Optional
from pathlib import Path
from tabulate import tabulate
from multiprocessing import Process, Queue
from torch.cuda import Stream
import torch.cuda
import torch
from concurrent.futures import ThreadPoolExecutor

mp.set_start_method('spawn', force=True)


matplotlib.use('TkAgg')
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(message)s',
    datefmt='%H:%M:%S'
)

logging.info("Yolo Detect run ...")

def timing_decorator(func):
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        print("Model : ",args[0].model.model_name)
        print(f"Function '{func.__name__}' started at {time.strftime('%H:%M:%S', time.localtime(start_time))}")
        
        result = func(*args, **kwargs)
        
        end_time = time.time()
        print(f"Function '{func.__name__}' ended at {time.strftime('%H:%M:%S', time.localtime(end_time))}")
        print(f"Total execution time: {end_time - start_time:.4f} seconds")

        return result
    return wrapper

@dataclass
class ImageSet:
    cam0_path: str
    cam2_path: str
    cam4_path: Optional[str] = None
    
    def load_image(self):
        door = cv2.imread(self.cam0_path, cv2.IMREAD_COLOR)
        under = cv2.imread(self.cam2_path, cv2.IMREAD_COLOR)
        cam4 = cv2.imread(self.cam4_path, cv2.IMREAD_COLOR) if self.cam4_path else None
        return door , under , cam4

class YoloDetector:
    def __init__(self, SCEN='1'):

        torch.backends.cudnn.benchmark = True # Î™®Îç∏Ïùò ÏûÖÎ†•ÌÅ¨Í∏∞ ÏùºÏ†ïÌï†Îïå ÏÜçÎèÑ ÏµúÏ†ÅÌôî
        torch.backends.cudnn.deterministic = False # Ïã§ÌóòÏùò Ïû¨ÌòÑÏÑ± Î∂àÌïÑÏöî ÏÜçÎèÑ / ÏÑ±Îä• Ïö∞ÏÑ†ÏàúÏúÑ
        torch.cuda.empty_cache()
        
        self.model = YOLO('yolo11x.pt').to('cuda')
        
        self.SOA = SeatOccupancyAnalyzer()
        self.conf_threshold = 0.25
        self.iou_threshold = 0.45
        
        self.SCEN = SCEN
        self.set_list = self._load_and_pair_images()
        self.seat_detector = SeatPositionDetector()
        self.visualizer = SitRecognition()
        self.pose_estimator = YoloPoseEstimator()
        self.segmentation = YoloSegmentation()
        self.tracking = YoloTracking()
        
    def _load_and_pair_images(self):
        """Ïù¥ÎØ∏ÏßÄ Î°úÎî© Î∞è ÌéòÏñ¥ÎßÅ"""
        try:
            cam0_list = natsorted(glob(f'collect/scne{self.SCEN}_0.5origin/cam0/*.jpg'))
            cam2_list = natsorted(glob(f'collect/scne{self.SCEN}_0.5origin/cam2/*.jpg'))
            cam4_list = natsorted(glob(f'collect/scne{self.SCEN}_0.5origin/cam4/*.jpg'))
            return self.pair(cam0_list, cam2_list, cam4_list , cam4_on = True)
        
        except Exception as e:
            logging.error(f"Ïù¥ÎØ∏ÏßÄ Î°úÎî© Ïã§Ìå®: {e}")
            return []

    def pair(self, cam0_list, cam2_list, cam4_list, cam4_on=True) -> list[ImageSet]:
        """@dataclassÎ•º Ïù¥Ïö©Ìïú Ïù¥ÎØ∏ÏßÄ ÎèôÍ∏∞Ìôî Îß§Ïπ≠"""
        buffer = []
        if not cam4_list:
            cam4_on = False
            
        try:
            cam0_dict = {Path(cam0_path).name : cam0_path for cam0_path in cam0_list}
            cam4_dict = {Path(cam4_path).name : cam4_path for cam4_path in cam4_list}

            for path in cam2_list:
                name = Path(path).name
                cam0_path = None
                cam4_path = None

                try:
                    if str(self.SCEN) in ['2', '3', '3x']:
                        index = Path(name).stem
                        adjusted_name = f"{str(int(index) + 10)}.jpg"
                        cam0_path = cam0_dict.get(adjusted_name)
                        cam4_path = cam4_dict.get(name)
                        
                    else:
                        cam0_path = cam0_dict.get(name)
                        cam4_path = cam4_dict.get(name)
                        
                    if cam4_on and cam0_path and cam4_path:
                        buffer.append(ImageSet(
                            cam0_path=cam0_path,
                            cam2_path=path,
                            cam4_path=cam4_path
                        ))
                    elif not cam4_on and cam0_path:
                        buffer.append(ImageSet(
                            cam0_path=cam0_path,
                            cam2_path=path
                        ))

                except ValueError as e:
                    logging.warning(f"ÌååÏùºÎ™Ö '{name}' Ï≤òÎ¶¨ Ï§ë Ïò§Î•ò Î∞úÏÉù: {e}")
                    continue

            return buffer

        except Exception as e:
            logging.error(f"ÌéòÏñ¥ÎßÅ Ï≤òÎ¶¨ Ï§ë Ïò§Î•ò Î∞úÏÉù: {e}")
            return []
        
    def test_run(self, image):
        import torch.autograd.profiler as profiler

        h, w, _ = image.shape
        
        current_time = time.time()
 
        result = self.model(image, classes=0, device='cuda:0', augment=False)
        result = result[0]


        output_image = image.copy()
        
        if hasattr(result, 'boxes') and len(result.boxes):
            boxes = result.boxes.cpu().numpy()
            for box in boxes:

                x1, y1, x2, y2 = map(int, box.xyxy[0])
                conf = float(box.conf[0])
                

                cv2.rectangle(output_image, (x1, y1), (x2, y2), color=(200,255,200) , thickness=2)
                
                conf_text = f'{conf:.2f}'
                cv2.putText(output_image, conf_text, (x1, y1 - 2), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)
                
        end_time = time.time()
        logging.info(f"test_run Ï≤òÎ¶¨ ÏãúÍ∞Ñ: {end_time - current_time:.4f} seconds")
        
        return output_image

    def main(self) -> None:
        try:
            self.set_list = self.set_list[750:]
            
            streams = {
                'cam0': torch.cuda.Stream(),
                'cam2': torch.cuda.Stream(),
                'cam2_2': torch.cuda.Stream(),
                'cam4': torch.cuda.Stream(),
            }
            
            for image_set in tqdm(self.set_list):
                current_time = time.time()
                
                door, under, cam4 = image_set.load_image()
                if door is None or under is None:
                    continue

                results = {}
 
                with torch.cuda.stream(streams['cam0']):
                    door_plane = fisheye2plane_GPU.run(door, -40)
                    tracking_result, filtering = self.tracking.tracking(door_plane, flag='cam0')
                    pred_result = self.prediction(door_plane, flag='cam0', filtering=filtering)
                    results['cam0'] = {'tracking': tracking_result, 'prediction': pred_result}
                    
 
                with torch.cuda.stream(streams['cam2']):
                    under_plane = fisheye2plane_GPU.run(under, -40)
                    tracking_result, filtering = self.tracking.tracking(under_plane, flag='cam2')
                    pred_result = self.prediction(under_plane, flag='cam2', filtering=filtering)
                    results['cam2'] = {'tracking': tracking_result, 'prediction': pred_result}

                with torch.cuda.stream(streams['cam2_2']):
                    under2_plane = fisheye2plane_GPU.run(under, 40, move=0, flag=False, cam_name='cam2_2')
                    tracking_result, filtering = self.tracking.tracking(under2_plane, flag='cam2_2')
                    pred_result = self.prediction(under2_plane, flag='cam2_2', filtering=filtering)
                    results['cam2_2'] = {'tracking': tracking_result, 'prediction': pred_result}
                
                if cam4 is not None:
                    cam4_time = time.time()
                    with torch.cuda.stream(streams['cam4']):
                        fisheye_time = time.time()
                        cam4_plane = fisheye2plane_GPU.run(cam4, -40)
                        logging.info(f"fisheye Ï≤òÎ¶¨ ÏãúÍ∞Ñ: {time.time() - fisheye_time:.4f} seconds")
                        c4 = self.test_run(cam4_plane)
                        results['cam4'] = {'prediction': c4}
                        cv2.namedWindow('cam4', cv2.WINDOW_NORMAL)
                        cv2.imshow('cam4', c4)
                        logging.info(f"cam4 Ï≤òÎ¶¨ ÏãúÍ∞Ñ: {time.time() - cam4_time:.4f} seconds")
                        
                torch.cuda.synchronize()

                self._process_results(results , current_time)

        except Exception as e:
            logging.error(f"Ïã§Ìñâ Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}")
            logging.error(traceback.format_exc())
            
        finally:
            cv2.destroyAllWindows()
            torch.cuda.empty_cache()
            
    def _process_results(self, results, current_time):
        for cam_name in ['cam0', 'cam2', 'cam2_2']:
            tracking_img = results[cam_name]['tracking']
            if tracking_img is not None and cam_name != 'cam0':
                cv2.namedWindow(f'{cam_name}_tracking', cv2.WINDOW_NORMAL)
                cv2.imshow(f'{cam_name}_tracking', tracking_img)

        cam0_pred = results['cam0']['prediction']
        cam2_pred = results['cam2']['prediction']
        cam2_2_pred = results['cam2_2']['prediction']
        
        door_prediction, seat_occupancy_count_cam0, detected_person_cordinate = cam0_pred
        under_prediction, seat_occupancy_count_cam2, _ = cam2_pred
        under2_prediction, seat_occupancy_count_cam2_2, _ = cam2_2_pred
        
        if all([seat_occupancy_count_cam2, seat_occupancy_count_cam0, seat_occupancy_count_cam2_2]):
            self.seat_detector.determine_seat_positions_cam2(seat_occupancy_count_cam2)
            self.seat_detector.camera_calibration(seat_occupancy_count_cam0, detected_person_cordinate)
            self.seat_detector.determine_seat_positions_cam2_2(seat_occupancy_count_cam2_2)
            #self.seat_detector.display_seat_status()

        #visualization = self.visualizer.visualize_seats(self.seat_detector.get_seat_status())
        
                
        end_time = time.time()
        logging.info(f"Frame Ï≤òÎ¶¨ ÏãúÍ∞Ñ: {end_time - current_time:.4f} seconds")
        
        for window, image in [
            ("cam0", door_prediction),
            ("cam2", under_prediction),
            ("cam2_2", under2_prediction),
            # ("visualization_result", visualization)
        ]:
            if image is not None and window != 'cam0':
                cv2.namedWindow(window, cv2.WINDOW_NORMAL)
                cv2.imshow(window, image)
                                
        key = cv2.waitKey(0) & 0xFF
        if key == ord('c'):
            while True:
                if cv2.waitKey(0) & 0xFF == ord('c'):
                    break
        
    def prediction(self, img: Union[np.ndarray, torch.Tensor], flag: str, filtering: List = []) -> np.ndarray:
        try:
            result = self.model.predict(img, classes=[0] , device='cuda:0' , iou=0.45 , conf=0.25 , augment=False)
            
            boxes = []
            scores = []
            for res in result:
                for box in res.boxes:
                    if int(box.cls) == 0:
                        x1, y1, w, h = box.xyxy[0].tolist()
                        score = box.conf
                        boxes.append([int(x1), int(y1), int(w - x1), int(h - y1)])
                        scores.append(float(score.detach().cpu().numpy()))

            filtered_boxes , _ = self.apply_nms(boxes, scores)
            nmx_boxes = list(map(self.nmx_box_to_cv2_loc, filtered_boxes))
            
            img_res  = self.draw_boxes(nmx_boxes, img.copy())
            img_res , filtering_boxes = self.filter_and_draw_boxes(nmx_boxes, img_res , filtering)
            
            if flag == 'cam0':
                img_res , row_counter , detected_person_cordinate = self.SOA.cam0_run(img_res, nmx_boxes , filtering_boxes)
            elif flag == 'cam2':
                img_res , row_counter , detected_person_cordinate = self.SOA.cam2_run(img_res, nmx_boxes , filtering_boxes)
            elif flag == 'cam2_2':
                img_res , row_counter , detected_person_cordinate = self.SOA.cam2_2run(img_res, nmx_boxes , filtering_boxes)
            
            return img_res , row_counter , detected_person_cordinate
        
        except Exception as e:    
            logging.error(f"ÏòàÏ∏° Ï§ë Ïò§Î•ò Î∞úÏÉù: {e}")
            raise e
        
    def nmx_box_to_cv2_loc(self, boxes):
        x1, y1, w, h = boxes
        x2 = x1 + w
        y2 = y1 + h
        return [x1, y1, x2, y2]

    def apply_nms(self, boxes, scores, iou_threshold=None):
        if iou_threshold is None:
            iou_threshold = self.iou_threshold
            
        if not boxes:
            return [], []
            
        # Î∞ïÏä§ÏôÄ Ï†êÏàòÎ•º numpy Î∞∞Ïó¥Î°ú Î≥ÄÌôò
        boxes = np.array(boxes)
        scores = np.array(scores)
        
        # confidence threshold Ï†ÅÏö©
        mask = scores > self.conf_threshold
        boxes = boxes[mask]
        scores = scores[mask]
        
        if len(boxes) == 0:
            return [], []
            
        # NMS Ïö©
        indices = cv2.dnn.NMSBoxes(boxes.tolist(), scores.tolist(), 
                                 self.conf_threshold, iou_threshold)
        
        if len(indices) > 0:
            if isinstance(indices, np.ndarray):
                indices = indices.flatten()
            else:
                indices = [i[0] for i in indices]
                
            return boxes[indices].tolist(), scores[indices].tolist()
        
        return [], []

    def draw_boxes(self , nmx_boxes: list[int], img) -> np.array:
        for box in nmx_boxes:
            x1, y1, x2, y2 = box
            center_x, center_y = (x1 + x2) // 2, (y1 + y2) // 2

            cv2.rectangle(img, (x1, y1), (x2, y2), (255, 255, 0), thickness=2)
            cv2.circle(img, (center_x, center_y), 5, (0, 0, 255), -1)  
            cv2.putText(img, f'({center_x}, {center_y})', 
                        (center_x, center_y), 
                        cv2.FONT_HERSHEY_DUPLEX, 
                        0.3, 
                        (255, 255, 255), 
                        1)
        return img
    
    def filter_and_draw_boxes(self, nmx_boxes: list[int], img, filtering) -> tuple:
        filtering_boxes = []
        for idx, box in enumerate(nmx_boxes):
            x1, y1, x2, y2 = box
            center_x, center_y = (x1 + x2) // 2, (y1 + y2) // 2
            
            if filtering:
                similarity_scores = []
                for filter_box in filtering:
                    score = self.is_similar_box([x1, y1, x2, y2], filter_box)
                    similarity_scores.append(score)
                
                if similarity_scores:
                    min_score_idx = similarity_scores.index(min(similarity_scores))
                    min_score = similarity_scores[min_score_idx]
                    
                    color = (0, 255, 0) if min_score < 0.3 else (0, 255, 255) if min_score < 0.6 else (255, 0, 0)
                    cv2.rectangle(img, (x1, y1), (x2, y2), color, thickness=2 if min_score < 0.6 else 3)
                    if min_score < 0.6:
                        filtering_boxes.append([x1, y1, x2, y2])                        
                        cv2.putText(img, f'Score: {min_score:.2f}', (x1, y1-10), cv2.FONT_HERSHEY_DUPLEX, 0.5, color, 1)

                else:
                    logging.error(f"Ïú†ÏÇ¨ÎèÑ 0.6Ïù¥ÏÉÅ Îß§Ïπ≠ÏïàÎê® ÏóêÎü¨ÏºÄÏù¥Ïä§ : {box}")
                    cv2.rectangle(img, (x1, y1), (x2, y2), (0, 0, 255), thickness=2) # Ïú†ÏÇ¨ÎèÑ 0.6Ïù¥ÏÉÅ Îß§Ïπ≠ÏïàÎê® ÏóêÎü¨ÏºÄÏù¥Ïä§
                    cv2.putText(img, f'Score: {min_score:.2f}', (x1, y1-10), cv2.FONT_HERSHEY_DUPLEX, 0.5, (0, 0, 255), 1)
            else:
                pass
            
           
            
        
        return img, filtering_boxes
    def is_similar_box(self, box1, box2):
        x1_1, y1_1, x2_1, y2_1 = box1
        x1_2, y1_2, x2_2, y2_2 = box2
        
        # Î∞ïÏä§Ïùò ÎÑàÎπÑ, ÎÜíÏù¥, Î©¥Ï†Å Í≥ÑÏÇ∞
        width1, height1 = x2_1 - x1_1, y2_1 - y1_1
        width2, height2 = x2_2 - x1_2, y2_2 - y1_2
        area1 = width1 * height1
        area2 = width2 * height2
        
        # ÌÅ¨Í∏∞ Ï∞®Ïù¥ ÎπÑÏú® Í≥ÑÏÇ∞
        width_diff = abs(width1 - width2) / max(width1, width2)
        height_diff = abs(height1 - height2) / max(height1, height2)
        area_diff = abs(area1 - area2) / max(area1, area2)
        
        # Ï§ëÏã¨Ï†ê Í≥ÑÏÇ∞ Î∞è Ï∞®Ïù¥
        center1 = ((x1_1 + x2_1) / 2, (y1_1 + y2_1) / 2)
        center2 = ((x1_2 + x2_2) / 2, (y1_2 + y2_2) / 2)
        center_dist = ((center1[0] - center2[0])**2 + (center1[1] - center2[1])**2)**0.5
        
        # Ïú†ÏÇ¨ÎèÑ Ï†êÏàò Í≥ÑÏÇ∞ (ÎÇÆÏùÑÏàòÎ°ù Îçî Ïú†ÏÇ¨)
        similarity_score = width_diff + height_diff + area_diff + (center_dist / 100)
        
        return similarity_score
    
class SeatOccupancyAnalyzer():
    """ Ï¢åÏÑù Ï∞©ÏÑùÏó¨Î∂ÄÎ•º Î∂ÑÏÑù Î∞è Ï≤òÎ¶¨"""    
    right_points = [
        (299, 0),    # ÏãúÏûëÏ†ê
        (299, 30),
        (299, 60),   # row1
        (299, 90),
        (299, 123),  # row2
        (297, 150),
        (295, 180),
        (293, 211),  # row3
        (292, 240),
        (291, 270),
        (291, 300),
        (290, 330),
        (290, 366),  # row4
        (289, 400),
        (288, 450),
        (287, 500),
        (287, 532),  # row5
        (288, 570),
        (289, 600),
        (290, 639)   # ÎÅùÏ†ê
    ]
    left_points = [
        (189, 0),    # ÏãúÏûëÏ†ê
        (189, 30),
        (189, 60),   # row1
        (182, 90),
        (175, 123),  # row2
        (160, 150),
        (146, 180),
        (138, 211),  # row3
        (120, 240),
        (100, 270),
        (89, 300),
        (80, 330),
        (74, 366),   # row4
        (60, 400),
        (40, 450),
        (20, 500),
        (3, 532),    # row5
        (2, 570),
        (1, 600),
        (0, 639)     # ÎÅùÏ†ê
    ]
    
    y_limit = [0, 60, 123, 211, 366, 532]
    cam0_x_limit = [0, 93, 253, 427, 599]
    cam0_y_limit = [350 , 125 , 440] # uppper lower  Î∞úÎ∞îÎã• ÌïòÌïúÏÑ†
        
    side_seat_limit_x , side_seat_limit_y = [ 530 , 200  ] 
    side_seat_threshold_x = 415  # Ï¢åÏÑù 9 10 ÌÜµÎ°úÏÇ¨Îûå ÎÖ∏Ïù¥Ï¶àÏ†úÍ±∞ ÎùºÏù∏

    seat_9_10_boundary_y = 325  # yÏ¢åÌëú 325Î•º Í∏∞Ï§ÄÏúºÎ°ú seat9Í≥º seat10ÏùÑ Íµ¨Î∂Ñ
    
    def cam2_run(self, img, boxes , filtering_boxes = []) :
        boxes = self.remove_filtering_boxes(boxes , filtering_boxes)
        h, w, _ = img.shape
        
        # Draw vertical line at x=585 in green
        cv2.line(img, (self.side_seat_limit_x, 0), (self.side_seat_limit_x, h), (0, 0, 255), 2)
        
        # Draw horizontal line at y=300 in yellow  
        cv2.line(img, (0, self.side_seat_limit_y), (w, self.side_seat_limit_y), (0, 255, 255), 2)
        cv2.line(img, (self.side_seat_threshold_x, 0), (self.side_seat_threshold_x, h), (0, 0, 255), 2)
        

        def find_intersection_x(y, points):
            for i in range(len(points) - 1):
                p1 = points[i]
                p2 = points[i + 1]
                y1, y2 = p1[1], p2[1]
                
                if min(y1, y2) <= y <= max(y1, y2):
                    x1, x2 = p1[0], p2[0]
                    if y1 != y2:
                        x = x1 + (x2 - x1) * (y - y1) / (y2 - y1)
                        return x
                    else:
                        return x1
            return None

        memory = []
        self.position_info = []

        side_seats_occupied = {'side_seats': {'seat9': 0, 'seat10': 0}}
        for i in boxes:
            x1, y1, x2, y2 = i
            center_x = (x1 + x2) // 2
            center_y = (y1 + y2) // 2
            
            if self.__class__.side_seat_limit_x <= x2 and y2 >= self.__class__.side_seat_limit_y and x1 >= self.__class__.side_seat_threshold_x:
                if y2 <= self.__class__.seat_9_10_boundary_y:
                    side_seats_occupied['side_seats']['seat9'] = 1
                    cv2.circle(img, (x2, y2), 5, (0, 255, 0), -1)
                    continue
                
                else:
                    side_seats_occupied['side_seats']['seat10'] = 1 
                    cv2.circle(img, (x2, y2), 5, (0, 255, 0), -1)
                    continue
                
            if  ( not center_x >= 300 and not y2 >= 570): # 570 cam2ÏóêÏÑú ÎßàÏßÄÎßâÌñâ Ï†úÏô∏ 
                idx = min(range(len(self.__class__.y_limit)), key=lambda i: abs(self.__class__.y_limit[i] - y1))
                memory.append(idx)
                
                left_x = find_intersection_x(center_y, self.__class__.left_points)
                right_x = find_intersection_x(center_y, self.__class__.right_points)
                
                if left_x is not None and right_x is not None:
                    dist_to_left = abs(center_x - left_x)
                    dist_to_right = abs(center_x - right_x)
                    position = "LEFT" if dist_to_left < dist_to_right else "RIGHT"
                    self.position_info.append((idx, position))
                    
                    cv2.circle(img, (int(left_x), center_y), 3, (255, 255, 0), -1)
                    cv2.circle(img, (int(right_x), center_y), 3, (255, 255, 0), -1)
                    
                    cv2.putText(img, f"{position} ({dist_to_left:.1f}, {dist_to_right:.1f})", 
                               (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 
                               0.5, (0, 255, 255), 1)

        row_counter = Counter(memory)
        row_counter = dict(sorted(row_counter.items(), key=lambda x: x[0]))

        position_details = {}
        for idx, pos in self.position_info:
            if idx not in position_details:
                position_details[idx] = {"LEFT": 0, "RIGHT": 0}
            position_details[idx][pos] += 1


        for key in row_counter.keys():
            loc = self.__class__.y_limit[key]
            left_count = position_details.get(key, {"LEFT": 0})["LEFT"]
            right_count = position_details.get(key, {"RIGHT": 0})["RIGHT"]
            total_count = row_counter[key]
            cv2.putText(img, f"Row{key}:L:{left_count} R:{right_count} (T:{total_count})", 
                       (100, loc + 30), cv2.FONT_HERSHEY_DUPLEX, 0.7, (0, 0, 255))
            
        position_details = dict(sorted(position_details.items(), key=lambda x: x[0]))
        position_details['side_seat'] = side_seats_occupied['side_seats']
                
        if 0 in position_details: # '0' keyÎ•º 1Í≥º Î≥ëÌï© 0Ïó¥ÏùÄ 1ÎüâÎ≤ÑÏä§ÏóêÏÑú 1Ïó¥Í≥º ÎèôÏùºÌïòÍ≤å Ï≤òÎ¶¨
            buffer = position_details.pop(0)
            if 1 in position_details:
                position_details[1]['LEFT'] = max(buffer['LEFT'], position_details[1]['LEFT'])
                position_details[1]['RIGHT'] = max(buffer['RIGHT'], position_details[1]['RIGHT'])
            else: # 1Ïó¥Ïù¥ ÏóÜÏúºÎ©¥ key 1ÏÉùÏÑ±ÌõÑ 0Ïó¥Í∞íÏùÑ 1Ïó¥Í∞íÏúºÎ°ú Ï∂îÍ∞Ä
                position_details[1] = buffer
                
        for i in range(1,5):
            if i not in position_details:
                position_details[i] = {'LEFT': 0, 'RIGHT': 0}
        if 'side_seat' not in position_details:
            position_details['side_seat'] = {'seat9': 0, 'seat10': 0}
        
        numeric_keys = sorted(k for k in position_details.keys() if isinstance(k, int))
        string_keys = sorted(k for k in position_details.keys() if isinstance(k, str))

        position_details_sorting = {k: position_details[k] for k in numeric_keys + string_keys}

        return img, position_details_sorting , []
    
    def cam0_run(self, img, boxes , filtering_boxes = []):

        def draw_guide_lines(img, h, w):
            for x in self.cam0_x_limit:
                cv2.line(img, (x, 0), (x, h - 1), (255, 0, 0), 2)
            for y in self.cam0_y_limit:
                cv2.line(img, (0, y), (w - 1, y), (0, 0, 255), 2)
            
            cv2.line(img , (0 , SeatPositionDetector.get_cam0_right_left_boundary() ), (w - 1 , SeatPositionDetector.get_cam0_right_left_boundary() ), (0, 255, 0), 2 )   
            
            return img

        def is_valid_detection(y1, y2, center_y):
            return (center_y <= self.cam0_y_limit[0] and 
                    y1 >= self.cam0_y_limit[1] and 
                    y2 <= self.cam0_y_limit[2])

        def determine_row(x1, lower_limit, upper_limit, current_row):
            if abs(lower_limit - x1) <= abs(upper_limit - x1):
                return current_row
            return current_row - 1 if current_row > 1 else None
        
        boxes = self.remove_filtering_boxes(boxes , filtering_boxes)
        h, w, _ = img.shape
        img = draw_guide_lines(img, h, w)
        
        
        detected_person_coordinates = {f'row{i}': [] for i in range(1, 5)}
        detected_row_numbers = []

        for x1, y1, x2, y2 in boxes:
            center_x, center_y = (x1 + x2) // 2, (y1 + y2) // 2
            
            if not is_valid_detection(y1, y2, center_y):
                continue
            
            
            cv2.rectangle(img, (x1, y1), (x2, y2), (199, 180, 200), 2)
            

            for idx in range(len(self.cam0_x_limit) - 1):
                lower_limit = self.cam0_x_limit[idx]
                upper_limit = self.cam0_x_limit[idx + 1]
                
                if lower_limit <= x1 <= upper_limit:
                    current_row = 4 - idx
                    row = determine_row(x1, lower_limit, upper_limit, current_row)
                    
                    if row:
                        detected_row_numbers.append(row)
                        detected_person_coordinates[f'row{row}'].append([center_x, center_y])
                    break

        row_counter = Counter(detected_row_numbers)
        row_counter.update({i: 0 for i in range(1, 5)})  
        
        for row_num, count in row_counter.items():
            loc = self.cam0_x_limit[4 - row_num]
            cv2.putText(img, f"T:{count}", 
                       (loc + 30, 30), 
                       cv2.FONT_HERSHEY_DUPLEX, 
                       0.7, (0, 0, 255))

        return img, dict(sorted(row_counter.items())), detected_person_coordinates

    def remove_filtering_boxes(self , boxes , filtering_boxes):
        for box in filtering_boxes:
            boxes.remove(box)
        return boxes
    
    def cam2_2run(self, img, boxes , filtering_boxes):
        
        boxes = self.remove_filtering_boxes(boxes , filtering_boxes)
        h , w , _ = img.shape
        limit_y = [350 , 200 , 150]
        limit_x = [310 , 200] # Ïö∞Ï∏° Ï¢åÏ∏° side
        seat_occupancy = {'row1': {'left' : 0 , 'right' : 0 , 'side' : 0},
                          'row2' : 0 ,
                          'row3' : 0 ,}
        
        for y in limit_y:
            cv2.line(img , (0 , y) , (w - 1 , y) , (0, 255, 0) , 2) # Ïó¥ÏùÑ Íµ¨Î∂ÑÌïòÎäîÏÑ†
        for x in limit_x:
            cv2.line(img , (x , 0) , (x , h - 1) , (0, 255, 255) , 2) # ÎÖ∏Ïù¥Ï¶àÎ•º Ï†úÍ±∞ÌïòÎäîÏÑ†
        
        for i in boxes:
            x1 , y1 , x2 , y2 = list(map(int , i))
            
            center_x , center_y = (x1 + x2) // 2 , (y1 + y2) // 2
            
            closest_y = min(limit_y , key=lambda y: abs(y - y1))
            closest_y_idx = limit_y.index(closest_y)
            
            if x1 > limit_x[0]:
                if closest_y_idx == 0:
                    left_distance = abs(center_x - limit_x[0])
                    right_distance = abs(center_x - (w - 1))
                    position = "LEFT" if left_distance < right_distance else "RIGHT"

                    side = 'left' if position == "LEFT" else 'right'
                    seat_occupancy['row1'][side] += 1
                    
                        
                    cv2.putText(img, f"{position} ({left_distance:.1f}, {right_distance:.1f})", 
                               (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 
                               0.5, (0, 255, 255), 1)
                    
                    cv2.rectangle(img , (x1 , y1) , (x2 , y2) , (0 , 165 , 255) , 2)
                
                elif closest_y_idx == 1:
                    cv2.rectangle(img , (x1 , y1) , (x2 , y2) , (255 , 0 , 255) , 2)
                    seat_occupancy['row2'] = 1
                
                elif closest_y_idx == 2:
                    cv2.rectangle(img , (x1 , y1) , (x2 , y2) , (0 , 0 , 255) , 2)
                    seat_occupancy['row3'] = 1
            
            elif x2 < limit_x[1] and center_y > limit_y[0]:
                cv2.rectangle(img , (x1 , y1) , (x2 , y2) , (0, 165, 255) , 2)
                seat_occupancy['row1']['side'] = 1
        
        return img , seat_occupancy , []
        
    
class SeatPositionDetector:
    cam0_right_left_boundary = 270
    
    @classmethod
    def get_cam0_right_left_boundary(cls):
        return cls.cam0_right_left_boundary
    
    def __init__(self):
        self.seat_positions = self._initialize_seats()
        
    def _initialize_seats(self):
        # Î™®Îì† Ï¢åÏÑùÏùÑ FalseÎ°ú Ï¥àÍ∏∞Ìôî
        return {
            f'row{i}': {'left': False, 'right': False}
            for i in range(1, 5) 
        } | {'side_seat': {'seat9': False, 'seat10': False}}  | {
            'row5': {'left' : False , 'right' : False , 'side' : False},
            'row6': False,
            'row7': False
        }


    def determine_seat_positions_cam2_2(self, cam2_2_detections):
        """
        Args:
            cam2_2_detections : {'row1': {'left': 1, 'right': 1, 'side': 1}, 'row2': 1, 'row3': 1}
        """

        try:
            for key, value in cam2_2_detections.items():
                adjust_key_name = f'row{int(key[3 : ]) + 4}'
                
                if adjust_key_name == 'row5':
                    for position in ['left', 'right', 'side']:
                        self.seat_positions['row5'][position] = bool(value.get(position, 0))
                    
                elif adjust_key_name in ['row6', 'row7']:
                    self.seat_positions[adjust_key_name] = bool(value)
                
        except Exception as e:
            logging.error(f"Ï¢åÏÑù Îß§Ìïë Ïò§Î•ò: {e}")
                

    def determine_seat_positions_cam2(self, cam2_detections):
        try:    
            
            for key, value in cam2_detections.items():
                if key == 'side_seat':
                    self._update_side_seats(value )
                    
              
                elif isinstance(key, int) and 1 <= key <= 4: # 1Ïó¥ ~ 4Ïó¥
                    row_name = f'row{key}'
                    self._update_seat_position(row_name, value )
                    
                elif isinstance(key , int) and  key == 0: # 0Ïó¥ÏùÄ Ï∂îÍ∞ÄÏ†ÅÏúºÎ°ú 1Ïó¥ÏúºÎ°ú Î∂ôÏûÑ
                    row_name = f'row{key + 1}'
                    self._update_seat_position(row_name, value )

        except Exception as e:
            logging.error(f"Ï¢åÏÑù ÏúÑÏπò Í≤∞Ï†ï Ï§ë Ïò§Î•ò Î∞úÏÉù: {e}")
            raise e
        
    def _update_seat_position(self, row_name, detection_info):
        left_occupied = detection_info.get('LEFT', 0) > 0
        right_occupied = detection_info.get('RIGHT', 0) > 0
        
        self.seat_positions[row_name]['left'] = left_occupied
        self.seat_positions[row_name]['right'] = right_occupied

    def _update_side_seats(self, side_seats):
        self.seat_positions['side_seat']['seat9'] = bool(side_seats.get('seat9', False))
        self.seat_positions['side_seat']['seat10'] = bool(side_seats.get('seat10', False))
        

    def camera_calibration(self, cam0_detections , detected_person_cordinate):
            """
            Ïπ¥Î©îÎùº Ï∫òÎ¶¨Î∏åÎ†àÏù¥ÏÖò Ï≤òÎ¶¨
            cam 0 ÏôÄ cam 2 Ïùò Ï¢åÏÑù ÏúÑÏπò Ï†ïÎ≥¥Î•º ÌÜµÌï¥ Ïπ¥Î©îÎùº Ï∫òÎ¶¨Î∏åÎ†àÏù¥ÏÖò Ï≤òÎ¶¨
            cam 0 ÌòïÌÉú : {0: 1, 1: 1, 2: 1, 3: 1 , 4: 1}
            cam 2 ÌòïÌÉú : {0: {'LEFT': 1, 'RIGHT': 0}, 1: {'LEFT': 1, 'RIGHT': 0}, 2: {'LEFT': 1, 'RIGHT': 0}, 3: {'LEFT': 1, 'RIGHT': 0}}
            detected_person_cordinate : {f'row{i}': [[center_x, center_y], [center_x, center_y]]}
            """
            # keyÎ•º row keyÎ°ú Î≥ÄÌôò
            row_detections = {}
            for key in cam0_detections:
                row_key = f'row{key}'
                row_detections[row_key] = cam0_detections[key]
                        
            for i in range(1,5): # Ïπ¥Î©îÎùº cam0 cam2Î•º Ïù¥Ïö©Ìïú Ï¢åÏÑù ÌåêÎã® ÏûëÏÑ±. # row1 ~ row4
                row_name = f'row{str(i)}'
                cam0_row_count = row_detections.get(row_name, 0) # Ìï¥ÎãπÌñâÏùò ÏÇ¨ÎûåÎ™ÖÏàò 
                cam2_row_count_left = self.seat_positions[row_name].get('left', 0)
                cam2_row_count_right = self.seat_positions[row_name].get('right', 0)
                                
                if cam0_row_count == 0 and cam2_row_count_left + cam2_row_count_right == 0:
                    # Ï†ïÏÉÅ ÏºÄÏù¥Ïä§: Ìï¥Îãπ Ïó¥Ïóê ÏïÑÎ¨¥ÎèÑ ÏóÜÏùå
                    self.seat_positions[row_name]['left'] = 0
                    self.seat_positions[row_name]['right'] = 0
                    
                elif cam0_row_count == 0 and (cam2_row_count_left + cam2_row_count_right > 0):
                    # cam2ÏóêÏÑúÎßå Í∞êÏßÄÎêú Í≤ΩÏö∞ - cam2 Í≤∞Í≥º ÏÇ¨Ïö©
                    self.seat_positions[row_name]['left'] = cam2_row_count_left 
                    self.seat_positions[row_name]['right'] = cam2_row_count_right
                    
                elif cam0_row_count == 1 and cam2_row_count_left + cam2_row_count_right == 0:
                    # cam0ÏóêÏÑúÎßå Í∞êÏßÄÎêú Í≤ΩÏö∞ (ÏÇ¨Í∞ÅÏßÄÎåÄ) - Ï¢åÏö∞ ÌåêÎã® Î∂àÍ∞ÄÎ°ú pass Í∑∏Îü¨ row3Î≤àÏùÄ ÏÜêÏû°Ïù¥Î°úÏù∏Ìï¥ cam0Ïù¥ ÌïòÎÇò cam2Í∞Ä 0ÏùºÎñÑ Ï¢åÎ°úÍ∞ÄÏ†ï
        
                    try:
                        if detected_person_cordinate[row_name] and len(detected_person_cordinate[row_name]) > 0:
                            _, cy = detected_person_cordinate[row_name][0]
                
                            if cy >= SeatPositionDetector.cam0_right_left_boundary: # ÏûÑÍ≥ÑÏ†êÎ≥¥Îã§ ÌÅ¨Îã§Î©¥ Ïò§Î•∏Ï™Ω
                                self.seat_positions[row_name]['right'] = 1
                            else:
                                self.seat_positions[row_name]['left'] = 1
                        else:
                            raise Exception(f"{row_name}Ïóê Ìïú Ï¢åÌëú Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§.")
                            
                    except Exception as e:
                        logging.error(f"Ïπ¥Î©îÎùº Ï∫òÎ¶¨Î∏åÎ†àÏù¥ÏÖò Ï§ë Ïò§Î•ò Î∞úÏÉù: {e}")
                        logging.error(f"ÏÉÅÏÑ∏ traceback:")
                        logging.error(traceback.format_exc())
                            
                elif cam0_row_count == 1 and cam2_row_count_left + cam2_row_count_right == 1:
                    # Ï†ïÏÉÅ ÏºÄÏù¥Ïä§: Ìïú Ïù¥ Ï†ïÌôïÌûà Í∞êÏßÄÎê® ÏòàÏô∏ÏÇ¨Ìï≠ Ï∂îÍ∞Ä cam2Îäî Ïò§Î•∏Ï™Ω 1Î™Ö cam0Îäî ÏôºÏ™Ω1Î™ÖÏùºÎïå 2Î™Ö on
                    _ , cy = detected_person_cordinate[row_name][0]
                    
                    if cy >= SeatPositionDetector.cam0_right_left_boundary: # ÏûÑÍ≥ÑÏ†êÎ≥¥Îã§ ÌÅ¨Îã§Î©¥ Ïò§Î•∏Ï™Ω
                        self.seat_positions[row_name]['right'] = 1
                    else:
                        self.seat_positions[row_name]['left'] = 1
                    
                    
                elif cam0_row_count == 1 and cam2_row_count_left + cam2_row_count_right == 2:
                    # cam2ÏóêÏÑú 2Î™Ö Í∞êÏßÄ - Ï¢åÏö∞ Î™® Ï∞©ÏÑùÏúº Ï≤òÎ¶¨
                    self.seat_positions[row_name]['left'] = 1
                    self.seat_positions[row_name]['right'] = 1
                    
                elif cam0_row_count == 2 and cam2_row_count_left + cam2_row_count_right == 0:
                    # cam0ÏóêÏÑú 2Î™Ö Í∞êÏßÄ - Ï¢åÏö∞ Î™®Îëê Ï∞©ÏÑùÏúºÎ°ú Ï≤òÎ¶¨
                    self.seat_positions[row_name]['left'] = 1
                    self.seat_positions[row_name]['right'] = 1
                    
                elif cam0_row_count == 2 and cam2_row_count_left + cam2_row_count_right == 1:
                    # cam0ÏóêÏÑú 2Î™Ö Í∞êÏßÄ - cam2 1Î™ÖÍ∞êÏßÄ -> cam0 ÌåêÎã® Ï∏°Î©¥ÏóêÏÑú Î≥¥Îäîcam0Í∞Ä 2Î™ÖÏã†Î¢∞ÏÑ±Ïù¥ ÎçîÎÜíÏùå (ÎÖ∏Ïù¥Ï¶àÏÉÅÌÉúÍ∞ÄÏïÑÎãàÎùºÎ©¥)
                    self.seat_positions[row_name]['left'] = 1
                    self.seat_positions[row_name]['right'] = 1
                    
                elif cam0_row_count == 2 and cam2_row_count_left + cam2_row_count_right == 2:
                    # ÏñëÏ™Ω Ïπ¥Î©îÎùºÏóêÏÑú Î™®Îëê 2Î™Ö Í∞êÏßÄ - (Ï†ïÏÉÅ ÏºÄÏù¥Ïä§)
                    self.seat_positions[row_name]['left'] = 1
                    self.seat_positions[row_name]['right'] = 1
                
                elif cam0_row_count > 2  or cam2_row_count_left + cam2_row_count_right > 2:
                    # ÏòàÏô∏ ÏºÄÏù¥Ïä§: 3Î™Ö Ïù¥ÏÉÅ Í∞êÏßÄ - Ï¢åÏö∞ Î™®Îëê Ï∞©ÏÑùÏúºÎ°ú Ï≤òÎ¶¨
                    logging.info(f"{row_name}Ïóê ÎåÄÌïú ÏòàÏô∏ ÏºÄÏù¥Ïä§: 3Î™Ö Ïù¥ÏÉÅ Í∞êÏßÄ")
                    self.seat_positions[row_name]['left'] = 1
                    self.seat_positions[row_name]['right'] = 1
                    
            cam0_detections.clear()
            cam0_detections.update(row_detections)
    
    def get_seat_status(self):
        return self.seat_positions

    def display_seat_status(self):
        """Ï¢åÏÑù ÏÉÅÌÉúÎ•º Ìëú ÌòïÌÉúÎ°ú Ï∂úÎ†•"""
        
        # ÏùºÎ∞ò Ï¢åÏÑù Îç∞Ïù¥ÌÑ∞ Íµ¨ÏÑ± (Row 1-4, 5-7)
        regular_seats = []
        side_seats = self.seat_positions['side_seat']
        
        # Row 1-2 Îç∞Ïù¥ÌÑ∞
        for i in range(1, 3):
            row = self.seat_positions[f'row{i}']
            regular_seats.append([
                f'Row {i}',
                'üü¢' if row['left'] else '‚ö´',
                'üü¢' if row['right'] else '‚ö´',
                '-'  # extra Ïπ∏
            ])
        
        # Row 3-4 Îç∞ÌÑ∞ (side_seat Ìè¨Ìï®)
        row3 = self.seat_positions['row3']
        regular_seats.append([
            'Row 3',
            'üü¢' if row3['left'] else '‚ö´',
            'üü¢' if row3['right'] else '‚ö´',
            'üü¢' if side_seats['seat9'] else '‚ö´'  # seat9Î•º Row 3 extraÏóê ÌëúÏãú
        ])
        
        row4 = self.seat_positions['row4']
        regular_seats.append([
            'Row 4',
            'üü¢' if row4['left'] else '‚ö´',
            'üü¢' if row4['right'] else '‚ö´',
            'üü¢' if side_seats['seat10'] else '‚ö´'  # seat10ÏùÑ Row 4 extraÏóê ÌëúÏãú
        ])
        
        # Row 5 Îç∞Ïù¥ÌÑ∞
        row5 = self.seat_positions['row5']
        regular_seats.append([
            'Row 5',
            'üü¢' if row5['left'] else '‚ö´',
            'üü¢' if row5['right'] else '‚ö´',
            'üü¢' if row5['side'] else '‚ö´'
        ])
        
        # Row 6-7 Îç∞Ïù¥ÌÑ∞
        for i in range(6, 8):
            regular_seats.append([
                f'Row {i}',
                '-',
                'üü¢' if self.seat_positions[f'row{i}'] else '‚ö´',
                '-'
            ])

        print("\n=== Î≤ÑÏä§ Ï¢åÏÑù ÏÉÅÌÉú ===")
        print(tabulate(regular_seats, 
                      headers=['Row', 'Left', 'Right', 'Extra'],
                      tablefmt='pretty'))

class YoloPoseEstimator:
    def __init__(self):
        self.pose_estimator = YOLO('yolo11x-pose.pt')
        # Ìïò ÌÇ§Ìè¨Ïù∏Ìä∏ Ïù∏Ïä§ Ï†ïÏùò
        self.lower_body_keypoints = {
            0: 'nose',          # ÏΩî
            1: 'neck',          # Î™©
            5: 'left_shoulder', # ÏôºÏ™Ω Ïñ¥Íπ®
            6: 'right_shoulder',# Ïò§Î•∏Ï™Ω Ïñ¥Íπ®
            7 : 'left_elbow',   # ÏôºÏ™Ω ÌåîÍøàÏπò
            8 : 'right_elbow',  # Ïò§Î•∏Ï™Ω ÌåîÍøàÏπò
            11: 'left_hip',     # ÏôºÏ™Ω ÏóâÎç©Ïù¥
            12: 'right_hip',    # Ïò§Î•∏Ï™Ω ÏóâÎç©Ïù¥
            13: 'left_knee',    # ÏôºÏ™Ω Î¨¥Î¶é
            14: 'right_knee',   # Ïò§Î•∏Ï™Ω Î¨¥Î¶é
            15: 'left_ankle',   # ÏôºÏ™Ω Î∞úÎ™©
            16: 'right_ankle',  # Ïò§Î•∏Ï™Ω Î∞úÎ™©
        }
        
        self.naming_keypoints = {}
        
        for key , value in self.lower_body_keypoints.items():
            name = value.replace('_', ' ')
            if '_' in value:
                direction = value.split('_')[0][:1]
                name = value.split('_')[1][:3]
                full_name = f"{direction}_{name}"
            else:
                full_name = name[:]
            
            self.naming_keypoints.update({key: full_name})

        self.KEYPOINT_INDEX = {v: k for k, v in self.lower_body_keypoints.items()}
        
        # Ïó∞Í≤∞ÏÑ† Ï†ïÏùòÎ•º ÌÅ¥ÎûòÏä§ ÏÜçÏÑ±ÏúºÎ°ú Ïù¥Îèô
        self.connections = [
            (13, 15),  # ÏôºÏ™Ω Î¨¥Î¶é-ÏôºÏ™Ω Î∞úÎ™©
            (14, 16),  # Ïò§Î•∏Ï™Ω Î¨¥Î¶é-Ïò§Î•∏Ï™Ω Î∞úÎ™©
            (11, 13),  # ÏôºÏ™Ω ÏóâÎç©Ïù¥-ÏôºÏ™Ω Î¨¥Î¶é
            (12, 14),  # Ïò§Î•∏Ï™Ω ÏóâÎç©Ïù¥-Ïò§Î•∏Ï™Ω Î¨¥Î¶é
        ]
        
        # ÏÉÅÏàò Ï†ïÏùò
        self.CONFIDENCE_THRESHOLD = 0.4
        self.LINE_COLOR = (0, 255, 0)    # Ï¥àÎ°ùÏÉâ
        self.POINT_COLOR = (255, 0, 0)   # ÌååÎûÄÏÉâ
        self.TEXT_COLOR = (0, 0, 255)    # Îπ®Í∞ÑÏÉâ
    def draw_skeleton(self, img, person_keypoints):
        """ÌÇ§Ìè¨Ïù∏Ìä∏ Í∞ÑÏùò Ïä§ÏºàÎ†àÌÜ§ Í∑∏Î¶¨Í∏∞"""
        for start_idx, end_idx in self.connections:
            # is_valid_keypoints Ìï®ÏàòÎ°ú Ìïú Î≤àÎßå Ïã†Î¢∞ÎèÑ Ï≤¥ÌÅ¨
            if self.is_valid_keypoints(person_keypoints, [start_idx, end_idx]):
                start_pos = tuple(map(int, person_keypoints[start_idx][:2]))
                end_pos = tuple(map(int, person_keypoints[end_idx][:2]))
                
                if start_pos[0] > 0 and start_pos[1] > 0 and end_pos[0] > 0 and end_pos[1] > 0: 
                    # ÏÑ† Í∑∏Î¶¨Í∏∞
                    cv2.line(img, start_pos, end_pos, self.LINE_COLOR, 2)
                    
                    # ÏÑ†Î∂ÑÏùò Ï§ëÏïôÏ†ê Í≥ÑÏÇ∞
                    mid_x = int((start_pos[0] + end_pos[0]) / 2)
                    mid_y = int((start_pos[1] + end_pos[1]) / 2)
                else:
                    continue
                # Ïã†Î¢∞ÎèÑ Ï†êÏàò Í∞ÄÏ†∏Í∏∞
                start_conf = float(person_keypoints[start_idx][2])
                end_conf = float(person_keypoints[end_idx][2])
                
                # Ïã†Î¢∞ÎèÑ ÌÖçÏä§Ìä∏ ÌëúÏãú (ÏÜåÏàòÏ†ê 2ÏûêÎ¶¨ÍπåÏßÄ)
                conf_text = f"{start_conf:.2f}/{end_conf:.2f}"
                cv2.putText(img, conf_text, (mid_x, mid_y), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, 
                           self.TEXT_COLOR, 1)

    def draw_keypoints(self, img, person_keypoints):
        """ÌÇ§Ìè¨Ïù∏Ìä∏ circle ÌëúÏãú """
        for idx, label in self.naming_keypoints.items():
            kp = person_keypoints[idx]
            if kp[2] > self.CONFIDENCE_THRESHOLD:
                x, y = map(int, kp[:2])
                cv2.circle(img, (x, y), 4, self.POINT_COLOR, -1)
                cv2.putText(img, label, (x + 5, y), 
                          cv2.FONT_HERSHEY_SIMPLEX, 0.5, 
                          self.TEXT_COLOR, 1)
                
    
    def is_valid_keypoints(self, keypoints, indices):
        """Ï£ºÏñ¥ÏßÑ ÌÇ§Ìè¨Ïù∏Ìä∏Îì§Ïù¥ Î™®Îëê Ïã†Î¢∞ÎèÑ ÏûÑÍ≥ÑÍ∞íÏùÑ ÎÑòÎäîÏßÄ ÌôïÏù∏"""
        for x , y , conf in keypoints[indices]:
            if conf < self.CONFIDENCE_THRESHOLD:
                return False
        return True
    
    def calculate_midpoint(self, keypoints, idx1, idx2):
        """Îëê ÌÇ§Ìè¨Ïù∏Ìä∏Ïùò Ï§ëÏ†ê Í≥ÑÏÇ∞"""
        if self.is_valid_keypoints(keypoints, [idx1, idx2]):
            return (keypoints[idx1][:2] + keypoints[idx2][:2]) / 2
        return None
    
    def calculate_direction_angle(self, person_keypoints):
        """
        'LEFT HIP' 'RIGHT HIP' 'LEFT KNEE' 'RIGHT KNEE' Î•º Ïù¥Ïö©ÌïòÏó¨ ÏÇ¨ÎûåÏù¥ Î∞îÎùºÎ≥¥Í≥† ÏûàÎäî Î∞©Ìñ• Í∞ÅÎèÑÎ•º Í≥ÑÏÇ∞Ìï©ÎãàÎã§.
        Returns:
            tuple: (angle, used_keypoints)
            angle: float - ÏàòÌïôÏ†Å Í∞ÅÎèÑ (-180 ~ 180ÎèÑ)
            used_keypoints: dict - ÏÇ¨Ïö©Îêú ÌÇ§Ìè¨Ïù∏Ìä∏ Ï†ïÎ≥¥
        """
        import math

        used_keypoints = {'method': None}  # Ïñ¥Îñ§ Î∞©Î≤ïÏúºÎ°ú Í≥ÑÏÇ∞ÎêòÏóàÎäîÏßÄ Ï†ÄÏû•

        # ÌÇ§Ìè¨Ïù∏Ìä∏ Ï∂îÏ∂ú
        left_hip = person_keypoints[self.KEYPOINT_INDEX.get('left_hip')].cpu().numpy()
        right_hip = person_keypoints[self.KEYPOINT_INDEX.get('right_hip')].cpu().numpy()
        left_knee = person_keypoints[self.KEYPOINT_INDEX.get('left_knee')].cpu().numpy()
        right_knee = person_keypoints[self.KEYPOINT_INDEX.get('right_knee')].cpu().numpy()
        left_ankle = person_keypoints[self.KEYPOINT_INDEX.get('left_ankle')].cpu().numpy()
        right_ankle = person_keypoints[self.KEYPOINT_INDEX.get('right_ankle')].cpu().numpy()

        # Ïã†Î¢∞ÎèÑ Ï≤¥ÌÅ¨
        if (all(hip[2] > self.CONFIDENCE_THRESHOLD and all(int(coord) > 0 for coord in hip[:2]) for hip in [left_hip, right_hip]) and
            all(knee[2] > self.CONFIDENCE_THRESHOLD and all(int(coord) > 0 for coord in knee[:2]) for knee in [left_knee, right_knee])):

            used_keypoints['method'] = 'hip_knee'
            used_keypoints.update({
                'left_hip': list(map(float, left_hip)),
                'right_hip': list(map(float, right_hip)),
                'left_knee': list(map(float, left_knee)),
                'right_knee': list(map(float, right_knee))
            })

            # ÏóâÎç©Ïù¥ Ï§ëÏ†ê Í≥ÑÏÇ∞
            hip_midpoint = [
                (left_hip[0] + right_hip[0]) / 2,
                (left_hip[1] + right_hip[1]) / 2
            ]

            # Î¨¥Î¶é Ï§ëÏ†ê Í≥ÑÏÇ∞
            knee_midpoint = [
                (left_knee[0] + right_knee[0]) / 2,
                (left_knee[1] + right_knee[1]) / 2
            ]

            # Î∞©Ìñ• Î≤°ÌÑ∞ Í≥ÑÏÇ∞ (ÏóâÎç©Ïù¥ÏóêÏÑú Î¨¥Î¶éÏúºÎ°ú Ìñ•ÌïòÎäî Î≤°ÌÑ∞)
            dx = knee_midpoint[0] - hip_midpoint[0]
            dy = knee_midpoint[1] - hip_midpoint[1]

            # Í∞ÅÎèÑ Í≥ÑÏÇ∞ (ÏàòÌèâÏÑ† Í∏∞Ï§Ä, Ïò§Î•∏Ï™ΩÏù¥ 0ÎèÑ)
            angle = math.degrees(math.atan2(dy, dx))
            
            # Í∞ÅÎèÑÎ•º -180 ~ 180 Î≤îÏúÑÎ°ú Ï°∞Ï†ï
            angle = angle % 360
            if angle > 180:
                angle -= 360
                
            return angle, used_keypoints
        
        elif (all(knee[2] > self.CONFIDENCE_THRESHOLD and all(int(coord) > 0 for coord in knee[:2]) for knee in [left_knee, right_knee]) and
              all(ankle[2] > self.CONFIDENCE_THRESHOLD and all(int(coord) > 0 for coord in ankle[:2]) for ankle in [left_ankle, right_ankle])):
                
            used_keypoints['method'] = 'knee_ankle'
            used_keypoints.update({
                'left_knee': list(map(float, left_knee)),
                'right_knee': list(map(float, right_knee)),
                'left_ankle': list(map(float, left_ankle)),
                'right_ankle': list(map(float, right_ankle))
            })
            
            knee_center = [
                (left_knee[0] + right_knee[0]) / 2,
                (left_knee[1] + right_knee[1]) / 2
            ]
            ankle_center = [
                (left_ankle[0] + right_ankle[0]) / 2,
                (left_ankle[1] + right_ankle[1]) / 2
            ]

            # Î¨¥Î¶éÏóêÏÑú Î∞úÎ™©ÏúºÎ°ú Í∞ÄÎäî Î∞©Ìñ• Î≤°ÌÑ∞ Í≥ÑÏÇ∞
            direction_vector = (ankle_center[0] - knee_center[0], ankle_center[1] - knee_center[1])

            # Î∞©Ìñ• Í∞ÅÎèÑ Í≥ÑÏÇ∞ (ÎùºÎîîÏïà -> ÎèÑ Îã®ÏúÑ Î≥ÄÌôò)
            angle = np.arctan2(direction_vector[1], direction_vector[0]) * (180 / np.pi)
            angle = angle % 360
            if angle > 180:
                angle -= 360
                
            return angle, used_keypoints
            
        else:
            return None, {'method': None}
        
    def calculate_head_direction_angle(self, person_keypoints):
        """ Î®∏Î¶¨ Î∞©Ìñ• Í∞ÅÎèÑ Í≥ÑÏÇ∞ """
        nose = person_keypoints[0].cpu().numpy()
        left_eye = person_keypoints[3].cpu().numpy()  # ÏôºÏ™Ω Îàà Ïù∏Îç±Ïä§ 3
        right_eye = person_keypoints[2].cpu().numpy() # Ïò§Î•∏Ï™Ω Îàà Ïù∏Îç±Ïä§ 2
        
        # ÏΩîÏôÄ ÏñëÏ™Ω ÎààÏùò Ïã†Î¢∞ÎèÑÏôÄ Ï¢åÌëú Ïú†Ìö®ÏÑ± Í≤ÄÏÇ¨
        if (nose[2] > self.CONFIDENCE_THRESHOLD and 
            left_eye[2] > self.CONFIDENCE_THRESHOLD and 
            right_eye[2] > self.CONFIDENCE_THRESHOLD and
            all(int(coord) > 0 for coord in nose[:2]) and
            all(int(coord) > 0 for coord in left_eye[:2]) and
            all(int(coord) > 0 for coord in right_eye[:2])):
            
            # ÏñëÏ™Ω ÎààÏùò Ï§ëÏ†ê Í≥ÑÏÇ∞
            eyes_center = np.array([
                (left_eye[0] + right_eye[0]) / 2,
                (left_eye[1] + right_eye[1]) / 2
            ])
            
            # Îàà Ï§ëÏ†êÏóêÏÑú ÏΩîÍπåÏßÄÏùò Î∞©Ìñ• Î≤°ÌÑ∞ Í≥ÑÏÇ∞
            head_vector = nose[:2] - eyes_center
            
            # Î∞©Ìñ• Í∞ÅÎèÑ Í≥ÑÏÇ∞ (ÎùºÎîîÏïà -> ÎèÑ)
            head_angle = math.degrees(math.atan2(head_vector[1], head_vector[0]))
            
            # Í∞ÅÎèÑÎ•º -180 ~ 180 Î≤îÏúÑÎ°ú Ï°∞Ï†ï
            head_angle = head_angle % 360
            if head_angle > 180:
                head_angle -= 360
            
            return head_angle
        else:
            return None
    
    def calculate_upper_body_direction(self, person_keypoints):
        """ ÏÉÅÏ≤¥ Î∞©Ìñ• Í∞ÅÎèÑ Í≥ÑÏÇ∞ """
       
        left_hip = person_keypoints[11].cpu().numpy()
        right_hip = person_keypoints[12].cpu().numpy()
        left_shoulder = person_keypoints[5].cpu().numpy()
        right_shoulder = person_keypoints[6].cpu().numpy()
        left_elbow = person_keypoints[7].cpu().numpy()
        right_elbow = person_keypoints[8].cpu().numpy()
        
        if left_hip[2] > self.CONFIDENCE_THRESHOLD and right_hip[2] > self.CONFIDENCE_THRESHOLD and \
           left_shoulder[2] > self.CONFIDENCE_THRESHOLD and right_shoulder[2] > self.CONFIDENCE_THRESHOLD and \
           all(int(coord) > 0 for coord in left_hip[:2]) and all(int(coord) > 0 for coord in right_hip[:2]) and \
           all(int(coord) > 0 for coord in left_shoulder[:2]) and all(int(coord) > 0 for coord in right_shoulder[:2]):
            
            # Ïñ¥Íπ® Ï§ëÏã¨Ï†êÍ≥º ÏóâÎç©Ïù¥ Ï§ëÏã¨Ï†ê Í≥ÑÏÇ∞
            shoulder_center = (left_shoulder[:2] + right_shoulder[:2]) / 2
            hip_center = (left_hip[:2] + right_hip[:2]) / 2
            
            # ÏÉÅÏ≤¥ Î∞©Ìñ• Î≤°ÌÑ∞ Í≥ÑÏÇ∞ (ÏóâÎç©Ïù¥ Ï§ëÏã¨ÏóêÏÑú Ïñ¥Íπ® Ï§ëÏã¨ÏúºÎ°ú)
            upper_body_vector = shoulder_center - hip_center
            
            # Î∞©Ìñ• Í∞ÅÎèÑ Í≥ÑÏÇ∞ (ÎùºÎîîÏïà -> ÎèÑ)
            upper_body_angle = math.degrees(math.atan2(upper_body_vector[1], upper_body_vector[0]))
            if left_shoulder[0] > right_shoulder[0]:
                upper_body_angle = 360 - upper_body_angle
                
            return upper_body_angle
        
        else:
            return None
        
    def judge_pose_by_angles(self, person_keypoints):
        LEFT_HIP = 11
        LEFT_KNEE = 13
        LEFT_ANKLE = 15

        # ÌÇ§Ìè¨Ïù∏Ìä∏ Ï∂îÏ∂ú Î∞è Ïã†Î¢∞ÎèÑ Ï≤¥ÌÅ¨
        left_hip = person_keypoints[LEFT_HIP].cpu().numpy()
        left_knee = person_keypoints[LEFT_KNEE].cpu().numpy()
        left_ankle = person_keypoints[LEFT_ANKLE].cpu().numpy()
        
        if left_hip[2] > self.CONFIDENCE_THRESHOLD and left_knee[2] > self.CONFIDENCE_THRESHOLD and \
           left_ankle[2] > self.CONFIDENCE_THRESHOLD and all(int(coord) > 0 for coord in left_hip[:2]) and \
           all(int(coord) > 0 for coord in left_knee[:2]) and all(int(coord) > 0 for coord in left_ankle[:2]):
            
            # ÏóâÎç©Ïù¥ÏóêÏÑú Î¨¥Î¶éÏúºÎ°úÏùò Î≤°ÌÑ∞
            hip_to_knee = left_knee[:2] - left_hip[:2]
            # Î¨¥Î¶éÏóêÏÑú Î∞úÎ™©ÏúºÎ°úÏùò Î≤°ÌÑ∞  
            knee_to_ankle = left_ankle[:2] - left_knee[:2]
            
            # Í∞Å Î≤°ÌÑ∞Ïùò Í∞ÅÎèÑ Í≥ÑÏÇ∞
            hip_knee_angle = math.degrees(math.atan2(hip_to_knee[1], hip_to_knee[0]))
            knee_ankle_angle = math.degrees(math.atan2(knee_to_ankle[1], knee_to_ankle[0]))
            
            # Îëê Í∞ÅÎèÑÏùò Ï∞®Ïù¥ Í≥ÑÏÇ∞
            angle_diff = hip_knee_angle - knee_ankle_angle

            # Í∞ÅÎèÑÎ•º -180 ~ 180 Î≤îÏúÑÎ°ú Ï°∞Ï†ï
            angle_diff = angle_diff % 360
            if angle_diff > 180:
                angle_diff -= 360
            return angle_diff
        
        else:
            return None
        
    def draw_angle_arrow(self, img, start_point, angle, length=50, color=(0, 165, 255), thickness=2):
        """
        Í∞ÅÎèÑÎ•º ÎÇòÌÉÄÎÇ¥Îäî ÌôîÏÇ¥ÌëúÎ•º Í∑∏Î¶ΩÎãàÎã§.
        Args:
            angle: -180 ~ 180 ÏúÑÏùò Í∞ÅÎèÑ
        """
        if angle is None:
            return img
        
        # Í∞ÅÎèÑÎ•º ÎùºÎîîÏïàÏúºÎ°ú Î≥ÄÌôò
        angle_rad = math.radians(angle)
        
        # ÎÅùÏ†ê Í≥ÑÏÇ∞ (yÏ∂ïÏù¥ ÏïÑÎûòÎ°ú Ï¶ùÍ∞ÄÌïòÎØÄÎ°ú sinÏóê -Î•º Í≥±Ìï®)
        end_x = int(start_point[0] + length * math.cos(angle_rad))
        end_y = int(start_point[1] + length * math.sin(angle_rad))
        end_point = (end_x, end_y)
        
        # ÌôîÏÇ¥Ìëú Í∑∏Î¶¨Í∏∞
        cv2.putText(img, f'{int(angle)}', (start_point[0], start_point[1] - 10), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 2)
        cv2.arrowedLine(img, start_point, end_point, color, thickness, tipLength=0.3)
        
        return img
    
    def PoseEstimation(self, img):
        try:
                        
            output_img = img.copy()
            zero_image = np.zeros_like(img, dtype=np.uint8)
            zero_image[ : , 300:, :] = img[:, 300:, :]
            img = zero_image.copy()

            results = self.pose_estimator(img,
                                        half=False,
                                        iou=0.5,
                                        conf=self.CONFIDENCE_THRESHOLD,
                                        device='cuda:0',
                                        classes=[0])

            for result in results:
                if not hasattr(result, 'keypoints') or result.keypoints is None:
                    continue
                if hasattr(result, 'boxes'):
                    boxes = result.boxes.data
                    for box in boxes:
                        x1, y1, x2, y2 = list(map(lambda x : int(x.cpu().numpy()) , box[:4]))
                        cv2.rectangle(output_img, (x1, y1), (x2, y2), (0, 165, 255), 5)
                    
                keypoints = result.keypoints.data
                xyxys = result.boxes.xyxy.data
                for index, (person_keypoints, xyxy) in enumerate(zip(keypoints, xyxys)):
                    x1, y1, x2, y2 = list(map(int, xyxy.cpu().numpy()))
                    center_x, center_y = int((x1+x2)//2), int((y1+y2)//2)   
                    cv2.rectangle(output_img, (x1, y1), (x2, y2), (0, 255, 0), 2)
                    cv2.putText(output_img, f'{index}', (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)
                    # Í∞ÅÎèÑ Í≥ÑÏÇ∞
                    head_angle = self.calculate_head_direction_angle(person_keypoints)
                    upper_body_angle = self.calculate_upper_body_direction(person_keypoints)

                    # y Ï¢åÌëú Ïò§ÌîÑÏÖã Ï¥àÍ∏∞Ìôî
                    y_offset = y1 - 20
                    
                    # Í∞Å Í∞ÅÎèÑÍ∞Ä Ïú†Ìö®Ìï† ÎïåÎßå ÌÖçÏä§Ìä∏ ÌëúÏãú
                    # if head_angle is not None:
                    #     cv2.putText(output_img,
                    #               f'Head Angle: {int(head_angle)}',
                    #               (x1, y_offset),
                    #               cv2.FONT_HERSHEY_SIMPLEX,
                    #               0.5,
                    #               (0,0,255),
                    #               2)
                    #     y_offset -= 20
                    
                    # if upper_body_angle is not None:
                    #     cv2.putText(output_img,
                    #               f'Upper Body: {int(upper_body_angle)}',
                    #               (x1, y_offset),
                    #               cv2.FONT_HERSHEY_SIMPLEX,
                    #               0.5,
                    #               (0,0,255),
                    #               2)
                    #     y_offset -= 20
                    

                    # angle = self.judge_pose_by_angles(person_keypoints) if self.judge_pose_by_angles(person_keypoints) is not None else None
                    # output_img = self.draw_angle_arrow(output_img, (center_x, center_y), angle, color=(128, 0, 128))
                    angle, used_keypoints = self.calculate_direction_angle(person_keypoints)
                    if angle is not None:
                        # ÌÖçÏä§Ìä∏ ÌëúÏãú
                        #print(f"index: {index} ,angle: {angle}, used_keypoints: {used_keypoints}")
                        
                        cv2.putText(output_img,
                                f'Lower Body: {int(angle)} ({used_keypoints["method"]})',
                                (x1, y_offset),
                                cv2.FONT_HERSHEY_SIMPLEX,
                                0.5,
                                (0,0,255),
                                2)
                        # ÌôîÏÇ¥Ìëú Í∑∏Î¶¨Í∏∞
                        output_img = self.draw_angle_arrow(output_img, (center_x, center_y), angle)
                    else:
                        angle = upper_body_angle if upper_body_angle is not None else None
                        if angle is not None:
                            cv2.putText(output_img,
                                    f'Upper Body: {int(angle)}',
                                    (x1, y_offset),
                                    cv2.FONT_HERSHEY_SIMPLEX,
                                    0.5,
                                    (0,0,255),
                                    2)
                            output_img = self.draw_angle_arrow(output_img, (center_x, center_y), angle, color=(255, 192, 203))
                            
                    self.draw_skeleton(output_img, person_keypoints)
                    self.draw_keypoints(output_img, person_keypoints)
            return output_img
            
        except Exception as e:
            logging.error(f"Pose estimation error: {str(e)}")
            return img

class YoloTracking:
    def __init__(self):
        self.tracking_model = YOLO('yolo11x.pt')
        self.test_model = YOLO('yolo11x.pt')
        self.visualize = SitRecognition()
   
        self.in_points =[
        (299, 0),    # ÏãúÏûëÏ†ê
        (299, 30),
        (299, 60),   # row1
        (299, 90),
        (299, 123),  # row2
        (297, 150),
        (295, 180),
        (293, 211),  # row3
        (292, 240),
        (291, 270),
        (291, 300),
        (290, 330),
        (290, 366),  # row4
        (289, 400),
        (288, 450),
        (287, 500),
        (287, 532),  # row5
        (288, 570),
        (289, 600),
        (290, 639)   # ÎÅùÏ†ê
    ]
        self.out_points = [
            (639, 290),  # idx 0
            (396, 232), (406, 282), (412, 308), (417, 330), 
            (423, 358), (430, 382), (436, 413), (444, 447), 
            (451, 475), (458, 499), (462, 521), (467, 538), 
            (470, 557), (477, 569), (481, 590), (485, 606), 
            (489, 620), (494, 638)
        ]
        

        self.under2_out_points = [(210,0),(227, 254), (225, 269), (220, 290), (214, 318), (208, 347), (204, 382), (197, 409), (192, 442), (181, 488), (176, 513), (163, 573), (154, 614), (150, 639)]
        self.under2_inpoints = 340
        
        
        # idx 1Î∂ÄÌÑ∞ ÎÅùÍπåÏßÄ xÏ¢åÌëúÏóê 30 ÎçîÌïòÍ∏∞ -> Í∏∞Ï°¥ÏùÄ Î≥µÎèÑÎùºÏù∏Ïóê ÎßûÏ∑ÑÎäîÎç∞ Ï°∞Ï†ïÏûëÏóÖ ÏßÑÌñâÌï® Ï°∞Í∏àÎçî ÏùòÏûêÏ™ΩÏúºÎ°ú Ï°∞Ï†ïÏßÑÌñâ
        for i in range(1, len(self.out_points)):
            x, y = self.out_points[i]
            self.out_points[i] = (x + 30, y)

        #self.cam0_out_points = [(636, 367), (559, 367), (542, 368), (527, 368), (512, 377), (493, 380), (468, 385), (451, 384), (435, 378), (420, 373), (401, 373), (376, 373), (358, 379), (338, 383), (319, 385), (305, 385), (289, 382), (275, 373), (260, 372), (241, 373), (228, 377), (210, 377), (187, 383), (164, 385), (142, 384), (129, 378), (119, 370), (109, 374), (100, 374), (75, 372), (51, 378), (25, 383), (4, 375)]
        self.cam0_out_points = [(639,415), (0,415)]
        
    def __create_track_line(self, img, flag): 
        """
        Ï∂îÏ†ÅÏÑ† ÎßàÏä§ÌÅ¨ ÏÉùÏÑ±
        Args:
            img (np.ndarray): ÏûÖÎ†• Ïù¥ÎØ∏ÏßÄ
            flag (str): Ïπ¥Î©îÎùº ÌîåÎûòÍ∑∏ ('cam0' or 'cam2')
        Returns:
            np.ndarray: ÎßàÏä§ÌÅ¨ Ïù¥ÎØ∏ÏßÄ
        """
        h, w = img.shape[:2]
        mask = np.zeros(shape=(h,w,3), dtype=np.uint8)
        
        # Ïπ¥Î©îÎùºÎ≥Ñ ÏÑ§Ï†ï
        track_config = {
            'cam0': {
                'points': [(w-1, h-1)] + self.cam0_out_points + [(0, h-1)],
                'reverse_points': False,
                'additional_mask': True  
            },
            'cam2': {
                'points': [(639, 0)] + self.in_points + self.out_points,
                'reverse_points': True,
                'additional_mask': False
            },
            'cam2_2': {
                'points': self.under2_out_points + [(340, h-1), (340, 0)],  # x=340 ÏÑ∏Î°úÏÑ† Ï∂îÍ∞Ä
                'reverse_points': False,
                'additional_mask' : False
            }
        }

        config = track_config[flag]
        points_list = config['points']
        
        if config['reverse_points']:
            points_list = points_list[:-len(self.out_points)] + self.out_points[::-1]

        if flag == 'cam2':
            points_list.append((639, 0))
        
        # Í∏∞Î≥∏ ÎßàÏä§ÌÅ¨ ÏÉùÏÑ±
        points = np.array(points_list, dtype=np.int32)
        cv2.fillPoly(mask, [points], (0,0,255))
        

        if flag == 'cam0' and config['additional_mask']:
            upper_limit = 135 # cam0 Î∞îÍπ•Ï∞ΩÎ¨∏ÏÇ¨Îûå Ï†úÍ±∞
            upper_points = np.array([
                [0, 0],     
                [w-1, 0],  
                [w-1, upper_limit],  
                [0, upper_limit]     
            ], dtype=np.int32)
            
            # Ï∂îÍ∞Ä ÎßàÏä§ÌÅ¨ Ï†ÅÏö©
            cv2.fillPoly(mask, [upper_points], (0,0,255))
        
        if flag == 'cam2_2':
            mask[ : 120 , :  ,  :] = 0
            
        return mask
    
    def tracking(self, img, flag):
        """
        Args:
            img (np.ndarray): ÏûÖÎ†• Ïù¥ÎØ∏ÏßÄ
            flag (str): Ïπ¥Î©îÎùº Î≤àÌò∏ ÌîåÎûòÍ∑∏ ('cam0' or 'cam2')
        Returns:
            np.ndarray: Ï∂úÎ†• Ïù¥ÎØ∏ÏßÄ
        """
        
        
        """
        Args:
           track_line_attr (str) : Î≥µÎèÑ ÌÜµÎ°ú mask Î≥ÄÏàòÎ™Ö
           create_track_line (function) : Î≥µÎèÑ ÌÜµÎ°ú mask ÏÉùÏÑ± Ìï®Ïàò
           movement_status_attr (dict) : ÏÇ¨Îûå ÏÉÅÌÉú Î≥ÄÏàò
           dynamic_status_attr (dict) : ÏÇ¨Îûå ÎèôÏ†Å/Ï†ïÏ†Å ÏÉÅÌÉú Î≥ÄÏàò
           center_points_attr (dict) : ÏÇ¨Îûå Î∞úÎ∞îÎã• Ï§ëÏã¨Ï†ê Î≥ÄÏàò
           prev_frame_ids_attr (set) : Ïù¥Ï†Ñ ÌîÑÎ†àÏûÑ Í∞ùÏ≤¥ id Ï†ÄÏû• Î≥ÄÏàò
           disappeared_counts_attr (dict) : ÏÇ¨ÎùºÏßÑ Í∞ùÏ≤¥ id ÌöüÏàò Ï†ÄÏû• Î≥ÄÏàò
           obj_colors_attr (dict) : Í∞ùÏ≤¥ idÎ≥Ñ ÏÉâÏÉÅ Ï†ÄÏû• Î≥ÄÏàò
           model (str) : Î™®Îç∏ Î≥ÄÏàòÎ™Ö
        """
        
        track_config = {
            'cam0': {
                'camera' : 'cam0',
                'track_line_attr': 'track_line_cam0',
                'create_track_line': self.__create_track_line,
                'movement_status_attr': 'movement_status_cam0',
                'dynamic_status_attr': 'dynamic_status_cam0',
                'center_points_attr': 'center_points_cam0',
                'prev_frame_ids_attr': 'prev_frame_ids_cam0',
                'disappeared_counts_attr': 'disappeared_counts_cam0',
                'obj_colors_attr': 'obj_colors_cam0',
                'model' : 'cam0_model'
            },
            'cam2': {
                'camera' : 'cam2',
                'track_line_attr': 'track_line_cam2',
                'create_track_line': self.__create_track_line,
                'movement_status_attr': 'movement_status_cam2',
                'dynamic_status_attr': 'dynamic_status_cam2',
                'center_points_attr': 'center_points_cam2',
                'prev_frame_ids_attr': 'prev_frame_ids_cam2',
                'disappeared_counts_attr': 'disappeared_counts_cam2',
                'obj_colors_attr': 'obj_colors_cam2',
                'model' : 'cam2_model'

            },
            'cam2_2': { 
                'camera' : 'cam2_2',
                'track_line_attr': 'track_line_cam2_2',
                'create_track_line': self.__create_track_line,
                'movement_status_attr': 'movement_status_cam2_2',
                'dynamic_status_attr': 'dynamic_status_cam2_2',
                'center_points_attr': 'center_points_cam2_2',
                'prev_frame_ids_attr': 'prev_frame_ids_cam2_2',
                'disappeared_counts_attr': 'disappeared_counts_cam2_2',
                'obj_colors_attr': 'obj_colors_cam2_2',
                'model' : 'cam2_2_model'
            }
        }
        logging.debug(f"track_config: {track_config}")
        config = track_config.get(flag , None)
        
        if config is None:
            return img , []
        
        if not hasattr(self , config['model']):
            setattr(self , config['model'] , YOLO('yolo11x.pt'))
    
        if not hasattr(self, config['track_line_attr']):
            setattr(self, config['track_line_attr'], self.__create_track_line(img.copy(), flag))
 
        for attr in ['movement_status_attr', 'dynamic_status_attr', 'center_points_attr', 
                    'obj_colors_attr', 'disappeared_counts_attr' ]:
            if not hasattr(self, config[attr]):
                setattr(self, config[attr], {})
        
        if not hasattr(self, config['prev_frame_ids_attr']):
            setattr(self, config['prev_frame_ids_attr'], set())
            
        results = getattr(self , config['model']).track(
            img.copy(),
            persist=True,
            classes=[0],
            half=False,
            device='cuda:0',
            iou=0.45,
            conf=0.25,
            imgsz=640,
            augment= False,
            tracker="bytetrack.yaml"  # ÎòêÎäî "botsort.yaml"
        )

        results = results[0]
                
        if hasattr(results, 'plot'):
            image_plot = results.plot()
            track_line = getattr(self, config['track_line_attr'])
            image_plot = cv2.addWeighted(image_plot, 0.8, track_line, 0.2, 0)
                
        movement_status = getattr(self, config['movement_status_attr']) # Í∞Å Í∞ùÏ≤¥(id) Î≥¥Î•ò / ÎèôÏ†Å / Ï†ïÏ†Å ÏÉÅÌÉú Ï†ÄÏû•
        current_frame_ids = set()
        filtering_moving_obj = []
        
        for obj in results.boxes:
            if obj.id is None:
                continue
                
            obj_id = int(obj.id)
            current_frame_ids.add(obj_id)
            
            x1, y1, x2, y2 = list(map(int, obj.xyxy[0].cpu().numpy()))
            x_center , y_center = (x1 + x2)//2, (y1 + y2)//2
            cv2.putText(image_plot, f'({x_center}, {y_center})', 
                        (x_center, y_center), 
                        cv2.FONT_HERSHEY_DUPLEX, 
                        0.3, 
                        (255, 255, 255), 
                        1)
            
            # Í∞ùÏ≤¥ ÏÉâÏÉÅ Ìï†Îãπ
            obj_colors = getattr(self, config['obj_colors_attr'])
            if obj_id not in obj_colors:
                obj_colors[obj_id] = (
                    random.randint(0,255),
                    random.randint(0,255),
                    random.randint(0,255))
            
            # ÏÇ¨Îûå Î∞úÎ∞îÎã• Ï†ÄÏû•
            center_points = getattr(self, config['center_points_attr'])
            if obj_id not in center_points:
                center_points[obj_id] = deque(maxlen=20)  
            center_points[obj_id].append(((x1 + x2)//2, y2))
            
            color = obj_colors[obj_id]
            
            
            cv2.circle(image_plot, (x_center, y2), 5, color, -1) # Î∞úÎ∞îÎã• Îã®Ïùº Ï∂úÎ†•
          
            # Î∞úÎ∞îÎã• Ïó∞ÏÜçÌï¥ÏÑú ÏãúÍ∞ÅÌôî Ï∂úÎ†• 
            # for id, points in center_points.items():
            #     color = obj_colors[id]
            #     for point in points:
            #         cv2.circle(image_plot, point, 5, color, -1)
            
            # ÏÇ¨Îûå Î∞úÎ∞îÎã• ÏúÑÏπò 5ÌîÑÎ†àÏûÑ Ï†ÄÏû•
            dynamic_status = getattr(self, config['dynamic_status_attr'])
            if obj_id not in dynamic_status:
                dynamic_status[obj_id] = deque(maxlen=5)
            
            dynamic_status[obj_id].append(((x1+x2)//2, y2))

            x_center = int((x1 + x2) / 2)
            y_center = int((y1 + y2) / 2)
            
            # Î∞úÎ∞îÎã•ÏúÑÏπòÎ¶Å ÌÜµÌïú ÎèôÏ†Å/Ï†ïÏ†Å ÏÉÅÌÉú Î∂ÑÏÑù
            if len(dynamic_status[obj_id]) >= 5: 
                coordinates = list(dynamic_status[obj_id])[-5:]
                points_in_mask = 0
            
                
                for coord in coordinates:
                    try:
                        x, y = map(int, coord)
                        track_line = getattr(self, config['track_line_attr'])
                        h, w = track_line.shape[:2]
       
                        x = max(0, min(x, w-1))  
                        y = max(0, min(y, h-1))  

                        # if track_line[y , x].any():
                        #     points_in_mask += 1   
                                             
                        if not track_config[flag]['camera'] in ['cam0' ,'cam2_2']:
                            if track_line[y, x].any():
                                points_in_mask += 1
                                
                        else:
                            box_area = (x2 - x1) * (y2 - y1)
                            box_mask = track_line[y1:y2, x1:x2]
                            overlap_pixel = np.sum(box_mask > 0)
                            overlap_ratio = overlap_pixel / box_area
                            
                            cv2.putText(image_plot, f"{overlap_ratio:.2f}", (x1, y1 + 20),
                            cv2.FONT_HERSHEY_DUPLEX, 1, (0, 255, 0), 2)
                            
                            if overlap_ratio > 0.1 and track_config[flag]['camera'] == 'cam0':
                                points_in_mask += 1
                                
                            elif overlap_ratio > 0.2 and track_config[flag]['camera'] == 'cam2_2':
                                points_in_mask += 1
                            
                    except Exception as e:
                        logging.warning(f"Ï¢åÌëú Ï≤òÎ¶¨ Ï§ë Ïò§Î•ò Î∞ú: {e}")
                        continue
                
                if points_in_mask >= 3:
                    movement_status[obj_id] = ("MOVING", points_in_mask)
                    cv2.putText(image_plot, "MOVING", (x_center, y_center),
                             cv2.FONT_HERSHEY_DUPLEX, 1, (0, 0, 255), 2)
                else:
                    movement_status[obj_id] = ("STATIC", points_in_mask)
                    cv2.putText(image_plot, "STATIC", (x_center, y_center),
                             cv2.FONT_HERSHEY_DUPLEX, 1, (0, 255, 0), 2)
                
                cv2.putText(image_plot, f"{points_in_mask}", (x1, y1 + 50),
                         cv2.FONT_HERSHEY_DUPLEX, 1, (0, 255, 255), 1)
            else:
                movement_status[obj_id] = ("PENDING", 0)
                cv2.putText(image_plot, "PENDING", (x_center, y_center),
                         cv2.FONT_HERSHEY_DUPLEX, 1, (0, 255, 255), 2)
            
            # MOVING ÏÉÅÌÉúÏù∏ Í∞ùÏ≤¥Ïùò x1,y1,x2,y2 Ï¢åÌëú Ï†ÄÏû•
            if movement_status[obj_id][0] == "MOVING":
                filtering_moving_obj.append([x1,y1,x2,y2])
                
        # ÏÇ¨ÎùºÏßÑ Í∞ùÏ≤¥ Ï≤òÎ¶¨
        prev_frame_ids = getattr(self, config['prev_frame_ids_attr']) 
        disappeared_ids = prev_frame_ids - current_frame_ids
        disappeared_counts = getattr(self, config['disappeared_counts_attr'])
        
        for disappeared_id in disappeared_ids:
            if disappeared_id not in disappeared_counts: # count ÌöüÏàò Ï≤¥ÌÅ¨ÏóêÏóÜÏúºÎ©¥ idÏôÄ ÌöüÏàò 1 Ï∂îÍ∞Ä 
                disappeared_counts[disappeared_id] = 1
            else:
                disappeared_counts[disappeared_id] += 1 # ÏÇ¨ÎùºÏßÑÌöüÏàòÍ∞Ä Ï°¥Ïû¨ÌïúÎã§Î©¥ Ìï¥ÎãπÍ∞ùÏ≤¥ idÌîÑÎ†àÏûÑÏóê ÌöüÏàò Ï∂îÍ∞Ä
                
            if disappeared_counts[disappeared_id] >= 5: # ÏÇ¨ÎùºÏßÑÌöüÏàòÍ∞Ä 5ÌîÑÎ†àÏûÑÏù¥ÏÉÅÏùºÎïå idÏÜåÎ©∏ ÏßÑÌñâ 
                dynamic_status = getattr(self, config['dynamic_status_attr'])
                if disappeared_id in dynamic_status:
                    del dynamic_status[disappeared_id]
                del disappeared_counts[disappeared_id]
        
        for current_id in current_frame_ids:
            if current_id in disappeared_counts:
                del disappeared_counts[current_id]
            else:
                if current_id not in disappeared_counts:
                    disappeared_counts[current_id] = 1
                    

        setattr(self, config['prev_frame_ids_attr'], current_frame_ids)
        
       
        # if flag == 'cam2':  # cam2ÏóêÏÑúÎßå ÏàòÌñâ
        #     filter_status = [id for id, (status, _) in movement_status.items() if status == "STATIC"]
        #     filter_boxes = []
        #     for obj in results.boxes:
        #         if obj.id in filter_status:
        #             x1, y1, x2, y2 = list(map(int, obj.xyxy[0].cpu().numpy()))
        #             cv2.rectangle(image_plot, (x1, y1), (x2, y2), (0,165,255), 3)
        #             filter_boxes.append([x1, y1, x2, y2])
            
        #     img, position_list, _ = self.location.cam2_run(img.copy(), filter_boxes)
        #     visualize_img = self.visualize.visualize_seats(position_list)
            
            
        #     cv2.namedWindow("visualize", cv2.WINDOW_NORMAL)
        #     cv2.imshow("visualize", visualize_img)
        
        return image_plot , filtering_moving_obj
    
    def test(self, img ):
        print("===============================test================================")
        results = self.tracking_model.track(
            img.copy(),
            persist=False,
            classes=[0],
            half=False,
            device='cuda:0',
            iou=0.45,
            conf=0.25,
            imgsz=640,
            augment=False,
            visualize = False,
        )
        result = results[0]
        if hasattr(result, 'plot'):
            image_plot = result.plot()
        else:
            image_plot = img
        
        return image_plot

class YoloSegmentation:
    colors = [
            (51, 255, 255),  # ÎÖ∏ÎûÄÏÉâ (Yellow)
            (51, 255, 51),   # Ï¥àÎ°ùÏÉâ (Green)
            (255, 51, 51),   # ÌååÎûÄÏÉâ (Blue)
            (153, 51, 51),   # ÎÇ®ÏÉâ (Navy)
            (255, 51, 255)   # Î≥¥ÎùºÏÉâ (Purple)
        ]
    def __init__(self):
        self.segmentation_model = YOLO('yolo11x-seg.pt')
  
    def segmentation(self, img):
        """
        Ïù¥ÎØ∏ÏßÄÏóêÏÑú Í∞ùÏ≤¥Î•º Í∞êÏßÄÌïòÍ≥† ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖòÏùÑ ÏàòÌñâÌï©ÎãàÎã§.
        
        Args:
            img (np.ndarray): ÏûÖÎ†• Ïù¥ÎØ∏ÏßÄ
            
        Returns:
            np.ndarray: ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò ÎßàÏä§ÌÅ¨Í∞Ä Ï†ÅÏö©Îêú Ïù¥ÎØ∏ÏßÄ
        """

        try:
            results = self.segmentation_model(img)
            result_img = img.copy()
            results = results[0]
            
            # ÌÅ¥ÎûòÏä§Î≥Ñ Í≥†Ïú† ÏÉâÏÉÅ ÏÉùÏÑ± (ÌÅ¥ÎûòÏä§ IDÎ•º ÌÇ§Î°ú ÏÇ¨Ïö©)
            class_colors = {}
            if hasattr(results, 'masks'):
                plot_image = results.plot()
                return plot_image
            
            if hasattr(results, 'masks') and results.masks is not None:
                # ÎßàÏä§ÌÅ¨ÏôÄ ÌÅ¥ÎûòÏä§ Ï†ïÎ≥¥ Í∞ÄÏ†∏Ïò§Í∏∞
                masks = results.masks.xyn
                classes = results.boxes.cls
                
                for mask, class_id in zip(masks, classes):
                    # ÌÅ¥ÎûòÏä§Î≥Ñ ÏÉâÏÉÅ Ìï†Îãπ (ÏóÜÏúºÎ©¥ ÏÉàÎ°ú ÏÉùÏÑ±)
                    if class_id not in class_colors:
                        class_colors[class_id] = YoloSegmentation.colors[class_id % len(YoloSegmentation.colors)]   

                    # ÎßàÏä§ÌÅ¨Î•º Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞Ïóê ÎßûÍ≤å Ï°∞Ï†ï
                    mask = mask.astype(np.uint8)
                    h, w = img.shape[:2]
                    mask = cv2.resize(mask, (w, h))
                    
                    # ÎßàÏä§ÌÅ¨ Ï†ÅÏö©
                    color = class_colors[class_id]
                    colored_mask = np.zeros_like(img)
                    colored_mask[mask > 0] = color
                    
                    # Ìà¨Î™ÖÎèÑÎ•º Ï†ÅÌïòÏó¨ ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÏôÄ Î∏îÎ†åÎî©
                    alpha = 0.5
                    mask_area = mask > 0
                    result_img[mask_area] = cv2.addWeighted(
                        result_img[mask_area], 
                        1 - alpha,
                        colored_mask[mask_area], 
                        alpha, 
                        0
                    )
                    
                    # ÌÅ¥ÎûòÏä§ Ïù¥Î¶Ñ ÌëúÏãú (ÎßàÏä§ÌÅ¨Ïùò ÏÉÅÎã®Ïóê)
                    mask_points = np.where(mask > 0)
                    if len(mask_points[0]) > 0:
                        top_point = (int(np.mean(mask_points[1])), int(np.min(mask_points[0])))
                        class_name = f"Class {int(class_id)}"  # ÌÅ¥ÎûòÏä§ Ïù¥Î¶ÑÏù¥ ÏûàÎã§Î©¥ Í∑∏Í≤ÉÏùÑ ÏÇ¨Ïö©
                        cv2.putText(
                            result_img,
                            class_name,
                            top_point,
                            cv2.FONT_HERSHEY_SIMPLEX,
                            0.5,
                            color,
                            2
                        )

            cv2.namedWindow("segmentation", cv2.WINDOW_NORMAL)
            cv2.imshow("segmentation", result_img)
            cv2.waitKey(0)
            cv2.destroyAllWindows()
            
            return result_img
            
        except Exception as e:
            logging.error(f"Segmentation Ï≤òÎ¶¨  Ïò§Î•ò Î∞úÏÉù: {str(e)}")
            logging.error(traceback.format_exc())
            return img

    
if __name__ == "__main__":
    try:
        C = YoloDetector()
        C.main()
    except Exception as e:
        logging.error(f"Î©îÏù∏ Ïã§Ìñâ Ï§ë Ïò§Î•ò Î∞úÏÉù: {e}")
        
        
        